# ðŸŽ Projet ETL OpenFoodFacts

[![Python](https://img.shields.io/badge/python-3.10-blue.svg)](https://www.python.org/downloads/)
[![Spark](https://img.shields.io/badge/spark-3.5.0-orange.svg)](https://spark.apache.org/)
[![MySQL](https://img.shields.io/badge/mysql-8.0-blue.svg)](https://www.mysql.com/)
[![Streamlit](https://img.shields.io/badge/streamlit-1.40+-red.svg)](https://streamlit.io/)

> **Projet d'IntÃ©gration de DonnÃ©es Massives (TRDE703)**
> Construction d'un pipeline Big Data complet pour l'analyse nutritionnelle de 4 millions de produits.

---

## ðŸŽ¯ Objectif

Transformer les donnÃ©es brutes d'OpenFoodFacts (**4,1M+ produits**, format CSV complexe) en un Datamart analytique propre et exploitable via un Dashboard interactif.

**Performance atteinte :** Traitement complet de 4,1 Millions de produits en **~35 minutes** sur machine locale (Windows).

---

## ðŸ—ï¸ Architecture Technique

```mermaid
graph LR
    A[OpenFoodFacts .csv.gz] -->|Spark Bronze| B(Extraction & Typage)
    B -->|Spark Silver| C(Nettoyage & Enrichissement)
    D[Ref. Pays] -.->|Broadcast Join| C
    C -->|Spark Gold| E(Chargement MySQL)
    E -->|SQL| F[(DataMart MySQL)]
    F -->|Connector| G[Dashboard Streamlit]
```
ðŸ› ï¸ Stack Technologique
ETL Engine : Apache Spark 3.5 (PySpark)

Storage : MySQL 8.0 (InnoDB)

Viz : Streamlit + Plotly

Langage : Python 3.10

OS : Compatible Windows / Linux / Mac

âœ¨ FonctionnalitÃ©s ClÃ©s
1. Extraction & Nettoyage (Bronze/Silver)
Lecture OptimisÃ©e : Chargement natif du format compressÃ© .gz.

Normalisation : Trim, Lower, Cast sÃ©curisÃ© (try_cast) pour gÃ©rer les donnÃ©es sales sans planter.

DÃ©doublonnage Intelligent : Conservation de la version la plus rÃ©cente via Window Function sur last_modified_t.

Enrichissement (Bonus) : Normalisation des pays via Broadcast Join (jointure optimisÃ©e en RAM).

Multilingue (Bonus) : RÃ©solution des noms de produits (coalesce FR > EN > Nom par dÃ©faut).

2. ContrÃ´le QualitÃ© AvancÃ©
Score de ComplÃ©tude : Calcul d'un score (0 Ã  1) basÃ© sur la prÃ©sence des informations clÃ©s (Marque, Nutriments, Nom).

RÃ¨gles MÃ©tier : DÃ©tection des incohÃ©rences (ex: Sucre > 100g, Energie < 0).

DÃ©tection Statistique (Bonus) : Identification des valeurs aberrantes (outliers) via la mÃ©thode IQR (Interquartile Range) sur les nutriments.

3. Chargement OptimisÃ© (Gold)
CompatibilitÃ© MySQL : Conversion automatique des types complexes (Array â†’ String).

Optimisation JDBC :

Utilisation de .repartition(8) avant Ã©criture pour Ã©viter la saturation mÃ©moire (Java Heap Space) et limiter le nombre de connexions ouvertes.

ParamÃ¨tres JDBC : batchsize=2000 et rewriteBatchedStatements=true pour la performance d'insertion.

ðŸ“‚ Structure du Projet
Projet-OpenFoodFacts-ETL/
â”‚
â”œâ”€â”€ etl/                    # Code Source PySpark
â”‚   â”œâ”€â”€ main.py             # Orchestrateur
â”‚   â”œâ”€â”€ bronze.py           # Extraction
â”‚   â”œâ”€â”€ silver.py           # Transformation & QualitÃ©
â”‚   â””â”€â”€ gold.py             # Chargement MySQL
â”‚
â”œâ”€â”€ dashboard.py            # Dashboard de visualisation Streamlit
â”œâ”€â”€ config.local.yaml       # Configuration (non versionnÃ©)
â”œâ”€â”€ etl.zip                 # Package zippÃ© pour Spark Submit
â””â”€â”€ README.md               # Documentation

ðŸš€ Installation & Lancement
PrÃ©requis
Java (JDK 8, 11 ou 17)

Python 3.8+

Serveur MySQL local

1. Configuration
CrÃ©ez un fichier config.local.yaml Ã  la racine :
spark:
  master: "local[*]"
  jars: "file:///C:/chemin/vers/mysql-connector-j-9.x.jar"

mysql:
  host: "localhost"
  port: 3306
  database: "openfoodfacts"
  user: "root"
  password: "votre_mot_de_passe"
  jdbc_url: "jdbc:mysql://localhost:3306/openfoodfacts?rewriteBatchedStatements=true"
  driver: "com.mysql.cj.jdbc.Driver"

openfoodfacts:
  raw_data_path: "data/raw/en.openfoodfacts.org.products.csv.gz"

bronze:
  # limit_rows: 1000  <-- Decommenter pour tester rapidement

2. Lancement du Pipeline ETL

# 1. CrÃ©ation de l'archive de code (pour les workers Spark)
Remove-Item .\etl.zip -Force
Compress-Archive -Path etl -DestinationPath etl.zip -Force

# 2. ExÃ©cution du Job Spark
# Adaptez les chemins vers python et le driver MySQL
spark-submit `
   --driver-memory 4g `
   --executor-memory 4g `
   --py-files etl.zip `
   etl/main.py `
   --config config.local.yaml

 Lancement du Dashboard (Bonus)

 python -m streamlit run dashboard.py

 ðŸ“Š MÃ©triques du Dernier RunLes mÃ©triques sont gÃ©nÃ©rÃ©es automatiquement dans metrics_last_run.json.Voici les rÃ©sultats sur le dataset complet (Run du 16/12/2025) :MÃ©triqueValeurProduits Lus (Input)4 170 401Doublons SupprimÃ©s2 592Produits Finaux (MySQL)4 167 809Score ComplÃ©tude Moyen73.28 %Anomalies DÃ©tectÃ©es15 103

 ðŸ§  StratÃ©gie d'Historisation (SCD2)
Note : Pour ce projet, un chargement de type "Snapshot" (Overwrite) a Ã©tÃ© implÃ©mentÃ© pour optimiser la performance du chargement initial (Bulk Load).

Pour une mise en production avec historisation Slowly Changing Dimension Type 2, la stratÃ©gie suivante serait appliquÃ©e :

Staging : Chargement des nouvelles donnÃ©es dans une table temporaire.

Comparaison : Jointure entre le Staging et le DataMart sur le code produit.

DÃ©tection de changement : Comparaison du product_hash (calculÃ© en phase Silver via SHA-256).

Mise Ã  jour :

Si le hash est diffÃ©rent :

UPDATE de l'ancienne ligne : set is_current=0, end_date=NOW().

INSERT de la nouvelle ligne : set is_current=1, start_date=NOW(), end_date=NULL.


COMMANDE A LANCER :

Remove-Item .\etl.zip -Force                                                                                                                                   
Compress-Archive -Path etl -DestinationPath etl.zip -Force
C:\spark\bin\spark-submit.cmd `     
>>    --master local[*] `
>>    --conf spark.pyspark.python=C:\Users\abraure\AppData\Local\Programs\Python\Python310\python.exe `
>>    --conf spark.pyspark.driver.python=C:\Users\abraure\AppData\Local\Programs\Python\Python310\python.exe `
>>    --jars C:\spark\jars\mysql-connector-j-9.5.0.jar `
>>    --driver-class-path C:\spark\jars\mysql-connector-j-9.5.0.jar `
>>    --conf spark.executor.extraClassPath=C:\spark\jars\mysql-connector-j-9.5.0.jar `
>>    --conf spark.driver.extraClassPath=C:\spark\jars\mysql-connector-j-9.5.0.jar `
>>    --driver-memory 6g `
>>    --executor-memory 6g `
>>    --py-files etl.zip `
>>    etl/main.py `
>>    --config config.local.yaml
